{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5134fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 决策树\n",
    "\n",
    "* 决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程\n",
    "* if-then规则的集合\n",
    "* 定义在特征空间与类空间上的条件概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541c30e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "决策树学习通常包括3个步骤：\n",
    "\n",
    "1. 特征选择\n",
    "1. 决策树的生成\n",
    "1. 决策树的修剪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f33527",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 决策树模型与学习\n",
    "\n",
    "### 决策树模型\n",
    "\n",
    "内部结点表示一个特征或属性，叶结点表示一个类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591821b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 决策树与if-then规则\n",
    "\n",
    "1. 由决策树的根结点到叶结点的每一条路径构建一条规则\n",
    "1. 路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511b80c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 决策树与条件概率分布\n",
    "\n",
    "* 将特征空间划分为互不相交的单元（cell）或区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布\n",
    "* 决策树的一条路径对应于划分中的一个单元\n",
    "* 决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c9795",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 决策树与条件概率分布\n",
    "\n",
    "* 各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率较大\n",
    "* 决策树分类时将该结点的实例强行分到条件概率大的那一类去\n",
    "* 当某个单元$c$的条件概率满足$P(Y = +1|X = c)>0.5$时，则认为这个单元属于正类，即落在这个单元的实例都被视为正例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3ea21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 决策树学习\n",
    "\n",
    "1. 开始，构建根结点，将所有训练数据都放在根结点\n",
    "2. 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类\n",
    "    * 如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去\n",
    "    * 如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点\n",
    "3. 如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止\n",
    "\n",
    "最后每个子集都被分到叶结点上，即都有了明确的类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331ddaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 决策树学习\n",
    "\n",
    "* 需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力\n",
    "* 决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eab625",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 特征选择\n",
    "\n",
    "### 特征选择问题\n",
    "\n",
    "* 如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的\n",
    "* 教材例5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fce799",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 教材例5.1\n",
    "\n",
    "* 希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类\n",
    "* 当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb1093",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 教材例5.1\n",
    "\n",
    "直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077b037",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 信息增益\n",
    "\n",
    "* 熵（entropy）是表示随机变量不确定性的度量\n",
    "* $X$是一个取有限个值的离散随机变量\n",
    "* $P(X=x_i)=p_i$\n",
    "* 随机变量$X$的熵定义为\n",
    "\n",
    "  \\begin{equation*}\n",
    "  H(X)=-\\sum_{i=1}^n p_i\\log p_i\n",
    "  \\end{equation*}\n",
    "  \n",
    "课堂提问：$0\\log 0$\n",
    "\n",
    "提示：$\\lim_{x\\to 0}x\\log x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c916051",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 信息增益\n",
    "\n",
    "* 熵只依赖于$X$的分布，而与$X$的取值无关\n",
    "* 所以也可将$X$的熵记作$H(p)$\n",
    "* 熵越大，随机变量的不确定性就越大\n",
    "\n",
    "  \\begin{equation*}\n",
    "  0\\le H(p)\\le \\log n\n",
    "  \\end{equation*}\n",
    "  \n",
    "课堂提问：（直觉上）为什么$0\\le H(p)\\le \\log n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa76f0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  Jensen’s inequality\n",
    "\n",
    "* $f$ convex\n",
    "* $0\\le \\theta\\le 1$\n",
    "* $f(\\theta x+(1-\\theta)y)\\le \\theta f(x)+(1-\\theta)f(y)$\n",
    "*  \n",
    "<center><img src=\"img/convex.png\" width=\"50%\"></center>\n",
    "\n",
    "课堂提问：不等式和图之间的联系"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32faef4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  Jensen’s inequality\n",
    "\n",
    "* $\\theta_1,\\dotsc,\\theta_k\\ge 0$, $\\theta_1+\\dotsb+\\theta_k=1$\n",
    "* \n",
    "  \\begin{equation*}\n",
    "  f(\\theta_1x_1+\\dotsb+\\theta_kx_k)\\le \\theta_1f(x_1)+\\dotsb+\\theta_kf(x_k)\n",
    "  \\end{equation*}\n",
    "  \n",
    "* \n",
    "  \\begin{equation*}\n",
    "  \\sqrt{ab}\\le \\frac{a+b}{2}\n",
    "  \\end{equation*}\n",
    "\n",
    "* \n",
    "  \\begin{equation*}\n",
    "  -\\log\\Bigl(\\frac{a+b}{2}\\Bigr)\\le \\frac{-\\log a-\\log b}{2}\n",
    "  \\end{equation*}\n",
    "  \n",
    "课堂提问：如何从$k=2$扩展到$k=3$，提示\n",
    "\n",
    "$$\n",
    "f(\\theta_1x_1+\\theta_2x_2+\\theta_3x_3)=f\\Bigl((1-\\theta_3)\\frac{1}{1-\\theta_3}(\\theta_1x_1+\\theta_2x_2)+\\theta_3x_3\\Bigr)\n",
    "$$\n",
    "  \n",
    "课堂提问：后两个不等式\n",
    "  \n",
    "课堂提问：（数学上）为什么$0\\le H(p)\\le \\log n$\n",
    "\n",
    "课堂提问：当随机变量只取两个值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efd9ae",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 补充知识\n",
    "\n",
    "\\begin{align*}\n",
    "H(p)&=-\\sum_i p_i\\log p_i\\\\\n",
    "&=\\sum_i p_i\\log\\frac{1}{p_i}\\\\\n",
    "&\\le\\log\\biggl(\\sum_i p_i\\frac{1}{p_i}\\biggr)=\\log n\n",
    "\\end{align*}\n",
    "\n",
    "上述proof依赖于$\\log x$为concav（和convex恰恰相反）。且可以将$p_i$理解为$\\theta_i$，$\\tfrac{1}{p_i}$理解为$x_i$。\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sqrt{ab}\\le \\frac{a+b}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "两边同时取$\\log$，从$\\log x$为concav来证明。\n",
    "\n",
    "连续变量：\n",
    "\n",
    "\\begin{align*}\n",
    "H(p)&=\\int_x p(x)\\log\\frac{1}{p(x)}\\mathop{}\\!\\mathrm{d} x\\\\\n",
    "&\\le \\log\\biggl(\\int_x p(x)\\frac{1}{p(x)}\\mathop{}\\!\\mathrm{d} x\\biggr)\n",
    "\\end{align*}\n",
    "\n",
    "对于$\\int_a^b$来说，上式为$\\log (b-a)$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e42fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 信息增益\n",
    "\n",
    "* 联合概率分布$P(X=x_i,Y=y_j)=p_{ij}$\n",
    "* 条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性\n",
    "* \n",
    "  \\begin{equation*}\n",
    "  H(Y|X)=\\sum_{i=1}^n p_iH(Y|X=x_i)\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d12ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 信息增益\n",
    "\n",
    "* 信息增益（information gain）表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度\n",
    "* \n",
    "  \\begin{equation*}\n",
    "  g(D,A)=H(D)-H(D|A)\n",
    "  \\end{equation*}\n",
    "\n",
    "* 一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息（mutual information）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220e827",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 信息增益\n",
    "\n",
    "* $H(Y)-H(Y|X)\\ge 0$\n",
    "* \n",
    "  \\begin{align}\n",
    "  H(Y)-H(Y|X)&=-\\sum_y p(y)\\log p(y)+\\sum_{x,y}p(x)p(y|x)\\log p(y|x)\\\\\n",
    "  &=-\\sum_{x,y}p(x,y)\\log p(y)+\\sum_{x,y}p(x,y)\\log p(y|x)\\\\\n",
    "  &=\\sum_{x,y}p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}\\\\\n",
    "  &=D_\\mathrm{KL}(P(X,Y)\\|P(X)P(Y))\n",
    "  \\end{align}\n",
    "  \n",
    "* \n",
    "  \\begin{align}\n",
    "  \\sum_i p_i\\log\\frac{q_i}{p_i}&\\le\\log\\sum_i p_i\\frac{q_i}{p_i}\\\\\n",
    "  &=\\log\\sum_i q_i=\\log 1=0\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580c66c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 信息增益\n",
    "\n",
    "* 对于数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益\n",
    "* 信息增益大的特征具有更强的分类能力\n",
    "* 根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$,计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877efff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 信息增益\n",
    "\n",
    "* 设有$K$个类$C_k$\n",
    "* $\\sum_{k=1}^K|C_k|=|D|$\n",
    "* 设特征$A$有$n$个不同的取值$\\{a_1,a_2,\\dotsc,a_n\\}$\n",
    "* 根据特征$A$的取值将$D$分为$n$个子集$D_1,D_2,\\dotsc,D_n$\n",
    "* 记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789f16e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 信息增益的算法\n",
    "\n",
    "1. 计算数据集$D$的经验熵$H(D)$\n",
    "\n",
    "   \\begin{equation*}\n",
    "   H(D)=-\\sum_{k=1}^K\\frac{|C_k|}{|D|}\\log\\frac{|C_k|}{|D|}\n",
    "   \\end{equation*}\n",
    "\n",
    "1. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$\n",
    "\n",
    "   \\begin{equation*}\n",
    "   H(D|A)=\\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}\\log\\frac{|D_{ik}|}{|D_i|}\n",
    "   \\end{equation*}\n",
    "   \n",
    "1. 计算信息增益\n",
    "\n",
    "   \\begin{equation*}\n",
    "   g(D,A)=H(D)-H(D|A)\n",
    "   \\end{equation*}\n",
    "   \n",
    "课堂提问：公式中各个项的含义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad581bc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 信息增益的算法\n",
    "\n",
    "教材例5.2\n",
    "\n",
    "课堂提问：板书计算表达式（示范之后）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2422785",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 信息增益比\n",
    "\n",
    "* 在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大\n",
    "* 反之，信息增益值会偏小\n",
    "* 信息增益比（information gain ratio）\n",
    "\n",
    "  \\begin{equation*}\n",
    "  g_R(D,A)=\\frac{g(D,A)}{H(D)}\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508178a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 决策树的生成\n",
    "\n",
    "### ID3算法\n",
    "\n",
    "1. 从根结点（root node）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点\n",
    "1. 再对子结点递归地调用以上方法，构建决策树\n",
    "1. 直到所有特征的信息增益均很小或没有特征可以选择为止\n",
    "1. 最后得到一个决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeccbe2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### ID3算法\n",
    "\n",
    "1. 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，返回$T$\n",
    "1. 若$A=\\emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$\n",
    "1. 否则，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$\n",
    "    * 如果$A_g$的信息增益小于阈值$\\epsilon$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$\n",
    "    * 否则，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，递归地构建子树（训练集：$D_i$，特征集：$A-\\{A_g\\}$）\n",
    "    \n",
    "教材例5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d2224",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### C4.5生成算法\n",
    "\n",
    "C4.5在生成的过程中，用信息增益比来选择特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429fe9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 决策树的剪枝\n",
    "\n",
    "* 过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树\n",
    "* 解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ec5c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 决策树的剪枝\n",
    "\n",
    "* 设树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{ik}$个，$H_t(T)$为叶结点$t$上的经验熵，$\\alpha\\ge 0$为参数\n",
    "* 决策树学习的损失函数可以定义为\n",
    "\n",
    "  \\begin{align*}\n",
    "  C_\\alpha(T)&=\\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha |T|\\\\\n",
    "  H_t(T)&=-\\sum_k\\frac{N_{tk}}{N_t}\\log\\frac{N_{tk}}{N_t}\n",
    "  \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a607ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 决策树的剪枝\n",
    "\n",
    "* \n",
    "  \\begin{equation*}\n",
    "  C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)\n",
    "  \\end{equation*}\n",
    "\n",
    "* \n",
    "  \\begin{equation*}\n",
    "  C_\\alpha(T)=C(T)+\\alpha|T|\n",
    "  \\end{equation*}\n",
    "  \n",
    "* $C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度\n",
    "* $|T|$表示模型复杂度\n",
    "* 参数$\\alpha\\ge 0$控制两者之间的影响：\n",
    "    * 较大的$\\alpha$促使选择较简单的模型（树）\n",
    "    * 较小的$\\alpha$促使选择较复杂的模型（树）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d27958",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 决策树的剪枝\n",
    "\n",
    "* 决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合\n",
    "* 而决策树剪枝通过优化损失函数还考虑了减小模型复杂度\n",
    "* 决策树生成学习局部的模型，而决策树剪枝学习整体的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba684b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 决策树的剪枝\n",
    "\n",
    "设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别是$C_\\alpha(T_B)$与$C_\\alpha(T_A)$，如果\n",
    "$$\n",
    "C_\\alpha(T_A)\\le C_\\alpha(T_B)\n",
    "$$\n",
    "则进行剪枝，即将父结点变为新的叶结点\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
