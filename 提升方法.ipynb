{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb776eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 提升方法\n",
    "\n",
    "* boosting\n",
    "* 在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a4228",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 提升方法AdaBoost算法\n",
    "\n",
    "* 对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好\n",
    "* “三个臭皮匠顶个诸葛亮”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb021a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 强可学习（strongly learnable）、弱可学习（weakly learnable）\n",
    "\n",
    "* 在概率近似正确（probably approximately correct，PAC）学习的框架中，\n",
    "    * 一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的\n",
    "    * 一个概念， 如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806095fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 等价\n",
    "\n",
    "* 非常有趣的是Schapire后来证明强可学习与弱可学习是等价的\n",
    "* 也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054b74f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 提升（boost）\n",
    "\n",
    "* 在学习中，如果已经发现了“弱学习算法”，那么能否将它提升（boost）为“强学习算法”\n",
    "* 最具代表性的是AdaBoost算法（AdaBoost algorithm）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53233a6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 提升方法\n",
    "\n",
    "* 提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器\n",
    "* 大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07fb5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 有两个问题需要回答\n",
    "\n",
    "* 一是在每一轮如何改变训练数据的权值或概率分布\n",
    "* 二是如何将弱分类器组合成一个强分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948200da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost的做法\n",
    "\n",
    "* 提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值\n",
    "* 弱分类器的组合：加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用\n",
    "\n",
    "  （加权多数表决）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7bcf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AdaBoost算法\n",
    "\n",
    "* 训练数据的权值分布\n",
    "\n",
    "  \\begin{equation*}\n",
    "  D_1=(w_{11},\\dotsc,w_{1i},\\dotsc,w_{1N})\n",
    "  \\end{equation*}\n",
    "  \n",
    "  $w_{1i}=\\tfrac{1}{N}$（初始）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b63b56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 基本分类器\n",
    "\n",
    "* 使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$\n",
    "\n",
    "  \\begin{equation*}\n",
    "  G_m(x):\\mathcal{X}\\to \\{-1,+1\\}\n",
    "  \\end{equation*}\n",
    "  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c815d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 分类误差率\n",
    "\n",
    "* $G_m(x)$在加权的训练数据集上的分类误差率是被$G_m(x)$误分类样本的权值之和\n",
    "\n",
    "  \\begin{equation*}\n",
    "  e_m=P(G_m(x_i)\\neq y_i)=\\sum_{i=1}^Nw_{mi}I(G_m(x_i)\\neq y_i)\n",
    "  \\end{equation*}\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30920b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 重要性\n",
    "\n",
    "* $G_m(x)$在最终分类器中的重要性\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\alpha_m=\\frac{1}{2}\\log\\frac{1-e_m}{e_m}\n",
    "  \\end{equation*}\n",
    "\n",
    "* 分类误差率越小的基本分类器在最终分类器中的作用越大\n",
    "\n",
    "  当$e_m\\le\\tfrac{1}{2}$时，$\\alpha_m\\ge 0$，并且$\\alpha_m$随着$e_m$的减小而增大\n",
    "  \n",
    "* 考虑$e^{\\alpha_m}=\\sqrt{\\tfrac{1-e_m}{e_m}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753f8d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 更新\n",
    "\n",
    "* 更新训练数据的权值分布为下一轮作准备\n",
    "\n",
    "  \\begin{equation*}\n",
    "  w_{m+1,i}=\n",
    "  \\begin{cases}\n",
    "  \\frac{w_{mi}}{Z_m}e^{-\\alpha_m},\\quad &G_m(x_i)=y_i\\\\\n",
    "  \\frac{w_{mi}}{Z_m}e^{\\alpha_m},\\quad &G_m(x_i)\\neq y_i\n",
    "  \\end{cases}\n",
    "  \\end{equation*}\n",
    "\n",
    "* 被基本分类器$G_m(x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf9dbf5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 加权表决\n",
    "\n",
    "* $M$个基本分类器的加权表决\n",
    "\n",
    "  \\begin{equation*}\n",
    "  f(x)=\\sum_{m=1}^M\\alpha_m G_m(x)\n",
    "  \\end{equation*}\n",
    "\n",
    "* 利用基本分类器的线性组合构建最终分类器\n",
    "\n",
    "  \\begin{equation*}\n",
    "  G(x)=\\mathrm{sign}(f(x))=\\mathrm{sign}\\biggl(\\sum_{m=1}^M\\alpha_mG_m(x)\\biggr)\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac576229",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AdaBoost的例子\n",
    "\n",
    "例8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fd490",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AdaBoost算法的训练误差分析\n",
    "\n",
    "**AdaBoost的训练误差界**\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{N}\\sum_{i=1}^N I(G(x_i)\\neq y_i)\\le \\prod_m Z_m\n",
    "\\end{equation*}\n",
    "\n",
    "* $G(x_i)\\neq y_i$表明$y_i$和$f(x_i)$的符号不一致\n",
    "* $I(\\cdot)$为0或1，而$1\\le e^{>0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559b7ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{equation*}\n",
    "\\frac{1}{N}\\sum_i e^{-\\sum_{i=1}^M\\alpha_my_iG_m(x_i)}\n",
    "\\end{equation*}\n",
    "\n",
    "* $\\tfrac{1}{N}$恰好为数据权值的初始分布\n",
    "* $e^{-\\sum_{i=1}^M}=\\prod_{i=1}^Me^-$\n",
    "* $\\sum_i w_{1i}\\prod_{i=1}^Me^{-\\alpha_my_iG_m(x_i)}$\n",
    "* $e^{-\\alpha_my_i}$可以从$\\prod$中提取出来，形成对数据权值分布的更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a0048",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**二类分类问题AdaBoost的训练误差界**\n",
    "\n",
    "直觉：$Z_m=(1-e_m)e^{-\\alpha_m}+e_me^{\\alpha_m}$\n",
    "\n",
    "\\begin{equation*}\n",
    "Z_m=2\\sqrt{e_m(1-e_m)}=\\sqrt{1-4\\gamma^2_m}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\prod_{m=1}^M\\sqrt{1-4\\gamma^2_m}\\le e^{-2\\sum_{m=1}^M\\gamma^2_m}\n",
    "\\end{equation*}\n",
    "\n",
    "$1+x\\le e^x$，对于实数$x$\n",
    "\n",
    "（等式仅当$x=0$）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e919f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 训练误差以指数数率下降\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{N}\\sum_{i=1}^N I(G(x_i)\\neq y_i)\\le e^{-2M\\gamma^2}\n",
    "\\end{equation*}\n",
    "\n",
    "* AdaBoost具有适应性\n",
    "* 能适应弱分类器各自的训练误差率\n",
    "* Ada是Adaptive的简写"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
