{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab37b34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 支持向量机\n",
    "\n",
    "* 支持向量机（support vector machines，SVM）是一种二类分类模型\n",
    "* 它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机\n",
    "* 支持向量机还包括核技巧，这使它成为实质上的非线性分类器\n",
    "* 支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming）的问题\n",
    "* 支持向量机的学习算法是求解凸二次规划的最优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e452757",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 支持向量机\n",
    "\n",
    "* 当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机\n",
    "* 当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization），也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机\n",
    "* 当训练数据线性不可分时，通过使用核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8987d344",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 支持向量机\n",
    "\n",
    "通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机\n",
    "\n",
    "课堂：演示`svm_kernel.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e8201",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 线性可分支持向量机与硬间隔最大化\n",
    "\n",
    "### 线性可分支持向量机\n",
    "\n",
    "* 假设给定一个特征空间上的训练数据集\n",
    "\n",
    "  \\begin{equation*}\n",
    "  T=\\{(x_1,y_1),(x_2,y_2),\\dotsc,(x_N,y_N)\\}\n",
    "  \\end{equation*}\n",
    "  \n",
    "  $y_i\\in\\mathcal{Y}=\\{+1,-1\\}$\n",
    "  \n",
    "* 当其$y_i=+1$时，称$x_i$为正例；当$y_i=-1$时，称$x_i$为负例\n",
    "\n",
    "* 再假设训练数据集是线性可分的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18675a5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 线性可分支持向量机\n",
    "\n",
    "* 学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类\n",
    "* 分离超平面对应于方程$w\\cdot x+b=0$\n",
    "* 分离超平面将特征空间划分为两部分，一部分是正类，一部分是负类\n",
    "* 法向量指向的一侧为正类，另一侧为负类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc2f59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 线性可分支持向量机\n",
    "\n",
    "* 感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个\n",
    "* 线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0636f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 线性可分支持向量机\n",
    "\n",
    "* 分类决策函数$f(x)=\\mathrm{sign}(w^*\\cdot x+b^*)$\n",
    "* (Source - Wikipedia)\n",
    "    * $H_1$不能把类别分开\n",
    "    * $H_2$可以，但只有很小的间隔\n",
    "    * $H_3$以最大间隔将它们分开\n",
    "\n",
    "<center><img src=\"img/SVM_Separating_Hyperplanes.svg\" width=\"40%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e9762",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 函数间隔和几何间隔\n",
    "\n",
    "* 一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度\n",
    "* 课本图7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4541190",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 函数间隔和几何间隔\n",
    "\n",
    "* 函数间隔（functional margin）：\n",
    "    * 样本点$(x_i,y_i)$：$\\hat{\\gamma}_i=y_i(w\\cdot x_i+b)$\n",
    "    * （课堂：内积的方式去理解）\n",
    "    * 训练数据集$T$：$\\hat{\\gamma}=\\min_{i=1,\\dots,N}\\hat{\\gamma}_i$\n",
    "* 只要成比例地改变$w$和$b$，例如将它们改为$2w$和$2b$，超平面并没有改变，但函数间隔却成为原来的2倍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a57743",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 函数间隔和几何间隔\n",
    "\n",
    "* 可以对分离超平面的法向量$w$加某些约束，如规范化，$\\|w\\|=1$，使得间隔是确定的\n",
    "* 几何间隔（geometric margin）\n",
    "  \n",
    "  \\begin{align*}\n",
    "  \\gamma_i&=y_i\\biggl(\\frac{w}{\\|w\\|}\\cdot x_i+\\frac{b}{\\|w\\|}\\biggr)\\\\\n",
    "  \\gamma &= \\min_{i=1,\\dots,N}\\gamma_i\n",
    "  \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44447dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 函数间隔和几何间隔\n",
    "\n",
    "* 超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔一般是实例点到超平面的带符号的距离（signed distance）\n",
    "* 当样本点被超平面正确分类时就是实例点到超平面的距离\n",
    "* 如果超平面参数$w$和$b$成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0abe1b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 间隔最大化\n",
    "\n",
    "* 支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面\n",
    "* 对线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机），但是几何间隔最大的分离超平面是唯一的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b34f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 间隔最大化\n",
    "\n",
    "间隔最大化的直观解释是：\n",
    "\n",
    "* 对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类\n",
    "* 也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开\n",
    "* 这样的超平面应该对未知的新实例有很好的分类预测能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41735979",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 间隔最大化\n",
    "\n",
    "**最大间隔分离超平面**\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{w,b}&\\quad \\gamma\\\\\n",
    "\\mathrm{s.t.}&\\quad y_i\\biggl(\\frac{w}{\\|w\\|}\\cdot x_i+\\frac{b}{\\|w\\|}\\biggr)\\ge \\gamma,\\quad i=1,2,\\dotsc,N\n",
    "\\end{align*}\n",
    "\n",
    "改写为：\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{w,b}&\\quad \\frac{\\hat{\\gamma}}{\\|w\\|}\\\\\n",
    "\\mathrm{s.t.}&\\quad y_i(w\\cdot x_i+b)\\ge \\hat{\\gamma},\\quad i=1,2,\\dotsc,N\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac09b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 间隔最大化\n",
    "\n",
    "**最大间隔分离超平面**\n",
    "\n",
    "* 函数间隔$\\hat{\\gamma}$的取值并不影响最优化问题的解\n",
    "* 事实上，假设将$w$和$b$按比例改变为$\\lambda w$和$\\lambda b$，这时函数间隔成为$\\lambda \\hat{\\gamma}$\n",
    "* 函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{w,b}&\\quad \\frac{1}{2}\\|w\\|^2\\\\\n",
    "\\mathrm{s.t.}&\\quad y_i(w\\cdot x_i+b)-1\\ge 0,\\quad i=1,2,\\dotsc,N\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde46db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 间隔最大化\n",
    "\n",
    "**最大间隔分离超平面**\n",
    "\n",
    "凸二次规划（convex quadratic programming）问题\n",
    "\n",
    "* 当目标函数是二次函数且约束函数是仿射函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f39c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 间隔最大化\n",
    "\n",
    "**支持向量和间隔边界**\n",
    "\n",
    "* 在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  y_i(w\\cdot x_i+b)-1=0\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 对$y_i=+1$的正例点，支持向量在超平面$H_1:w\\cdot x+b=1$\n",
    "* 对$y_i=-1$的负例点，支持向量在超平面$H_2:w\\cdot x+b=-1$\n",
    "* (Source - Wikipedia)\n",
    "\n",
    "<center><img src=\"img/Svm_max_sep_hyperplane_with_margin.png\" width=\"30%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a28d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 间隔最大化\n",
    "\n",
    "**支持向量和间隔边界**\n",
    "\n",
    "* 在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用\n",
    "    * 如果移动支持向量将改变所求的解\n",
    "    * 但是如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的\n",
    "* 支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b6959",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "* 对偶问题往往更容易求解\n",
    "* 自然引入核函数，进而推广到非线性分类问题\n",
    "* 定义拉格朗日函数：\n",
    "\n",
    "  $\\alpha_i\\ge 0,i=1,2,\\dotsc,N$\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  L(w,b,\\alpha)=\\frac{1}{2}\\|w\\|^2-\\sum_{i=1}^N\\alpha_iy_i(w\\cdot x_i+b)+\\sum_{i=1}^N\\alpha_i\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9368c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "* 原始问题的对偶问题是极大极小问题：\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\max_\\alpha\\min_{w,b} L(w,b,\\alpha)\n",
    "  \\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3e0b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "* 求$\\min_{w,b}L(w,b,\\alpha)$：\n",
    "\n",
    "  \\begin{align*}\n",
    "  \\nabla_w L(w,b,\\alpha)&=0\\\\\n",
    "  \\nabla_b L(w,b,\\alpha)&=0\n",
    "  \\end{align*}\n",
    "  \n",
    "  结果：\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\min_{w,b}L(w,b,\\alpha)=-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)+\\sum_{i=1}^N\\alpha_i\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16291252",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "对偶最优化问题\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_\\alpha\\quad &\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i\\\\\n",
    "\\text{s.t.}\\quad &\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "&\\alpha_i\\ge 0,\\quad i=1,2,\\dotsc,N\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4260505",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "对线性可分训练数据集，假设对偶最优化问题对$\\alpha$的解为$\\alpha^*$，可以由$\\alpha^*$求得原始最优化问题对$(w,b)$的解$w^*,b^*$\n",
    "\n",
    "\\begin{align*}\n",
    "w^*&=\\sum_{i=1}^N\\alpha^*_iy_ix_i\\\\\n",
    "b^*&=y_j-\\sum_{i=1}^N\\alpha^*_iy_i(x_i\\cdot x_j)\n",
    "\\end{align*}\n",
    "\n",
    "（存在下标$j$，使得$\\alpha^*_j>0$）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb53b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "线性可分支持向量机学习算法\n",
    "\n",
    "* 选择$\\alpha^*$的一个正分量$\\alpha^*_j>0$，计算\n",
    "\n",
    "\\begin{equation*}\n",
    "b^*=y_j-\\sum_{i=1}^N\\alpha^*_iy_i(x_i\\cdot x_j)\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110cf3a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "线性可分支持向量机学习算法\n",
    "\n",
    "* 求得分离超平面：\n",
    "\n",
    "  \\begin{equation*}\n",
    "  w^*\\cdot x+b^*=0\n",
    "  \\end{equation*}\n",
    "\n",
    "* 分类决策函数：\n",
    "\n",
    "  \\begin{align*}\n",
    "  f(x)&=\\mathrm{sign}(w^*\\cdot x+b^*)\\\\\n",
    "  f(x)&=\\mathrm{sign}\\biggl(\\sum_{i=1}^N\\alpha^*_iy_i(x_i\\cdot x)+b^*\\biggr)\n",
    "  \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c0cffc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "支持向量\n",
    "\n",
    "将训练数据集中对应于$\\alpha^*_i>0$的样本点$(x_i,y_i)$的实例$x_i\\in\\mathbf{R}^n$称为支持向量\n",
    "\n",
    "* 由KKT互补条件可知：\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\alpha^*_i(y_i(w^*\\cdot x_i+b^*)-1)=0,\\quad i=1,2,\\dotsc,N\n",
    "  \\end{equation*}\n",
    "\n",
    "* 对应于$\\alpha^*_i>0$的实例$x_i$，有\n",
    "\n",
    "  \\begin{equation*}\n",
    "  y_i(w^*\\cdot x_i+b^*)-1=0\n",
    "  \\end{equation*}\n",
    "\n",
    "  （$x_i$—定在间隔边界上）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47debf21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "课本例7.1及例7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087468a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 线性支持向量机与软间隔最大化\n",
    "\n",
    "### 线性支持向量机\n",
    "\n",
    "* 在现实问题中，训练数据集往往是线性不可分的，即在样本中出现噪声或特异点\n",
    "* 需要修改硬间隔最大化，使其成为软间隔最大化\n",
    "* 线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e4ce68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 线性支持向量机\n",
    "\n",
    "* 可以对每个样本点$(x_i,y_i)$引进一个松弛变量$\\xi_i\\ge 0$，使函数间隔加上松弛变量大于等于1\n",
    "\n",
    "  \\begin{equation*}\n",
    "  y_j(w\\cdot x_i+b)\\ge 1-\\xi_i\n",
    "  \\end{equation*}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b69ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 线性支持向量机\n",
    "\n",
    "* 目标函数由原来的$\\tfrac{1}{2}\\|w\\|^2$变成\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\frac{1}{2}\\|w\\|^2+C\\sum_{i=1}^N\\xi_i\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 使$\\tfrac{1}{2}\\|w\\|^2$尽量小即间隔尽量大，同时使误分类点的个数尽量小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4013e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 线性支持向量机\n",
    "\n",
    "* 软间隔最大化\n",
    "\n",
    "  \\begin{align*}\n",
    "  \\min_{w,b,\\xi}\\quad &\\frac{1}{2}\\|w\\|^2+C\\sum_{i=1}^N\\xi_i\\\\\n",
    "  \\text{s.t.}\\quad &y_i(w\\cdot x_i+b)\\ge 1-\\xi_i,\\quad i=1,2,\\dotsc,N\\\\\n",
    "  &\\xi_i\\ge 0,\\quad i=1,2,\\dotsc,N\n",
    "  \\end{align*}\n",
    "  \n",
    "* 线性支持向量机包含线性可分支持向量机\n",
    "* 由于现实中训练数据集往往是线性不可分的，线性支持向量机具有更广的适用性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21fe4b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\alpha}\\quad &\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i\\\\\n",
    "\\text{s.t.}\\quad &\\sum_{i=1}^N\\alpha_iy_i=0\\\\\n",
    "&0\\le\\alpha_i\\le C,\\quad i=1,2,\\dotsc,N\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64782e5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "L(w,b,\\xi,\\alpha,\\mu)=&\\frac{1}{2}\\|w\\|^2+C\\sum_{i=1}^N\\xi_i\\\\\n",
    "&-\\sum_{i=1}^N\\alpha_i(y_i(w\\cdot x_i+b)-1+\\xi_i)-\\sum_{i=1}^N\\mu_i\\xi_i\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "（其中，$\\alpha_i\\ge0,\\mu_i\\ge 0$）\n",
    "\n",
    "*  \n",
    "\\begin{equation*}\n",
    "\\nabla_{\\xi_i} L(w,b,\\xi,\\alpha,\\mu)=0\n",
    "\\end{equation*}\n",
    "* \n",
    "\\begin{equation*}\n",
    "C-\\alpha_i-\\mu_i=0\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec346c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 学习的对偶算法\n",
    "\n",
    "线性支持向量机学习算法\n",
    "\n",
    "* 选择$\\alpha^*$的一个分量$\\alpha^*_j$适合条件$0<\\alpha^*_j<C$，计算\n",
    "\n",
    "\\begin{equation*}\n",
    "b^*=y_j-\\sum_{i=1}^N\\alpha^*_iy_i(x_i\\cdot x_j)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b6217",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 支持向量\n",
    "\n",
    "1. 若$0<\\alpha^*_i<C$，则$\\xi_i=0$，支持向量$x_i$恰好落在间隔边界上\n",
    "1. 若$\\alpha^*_i=C,0<\\xi_i<1$，则分类正确，$x_i$在间隔边界与分离超平面之间\n",
    "1. 若$\\alpha^*_i=C,\\xi_i=1$，则$x_i$在分离超平面上\n",
    "1. 若$\\alpha^*_i=C,\\xi_i>1$，则$x_i$位于分离超平面误分一侧\n",
    "\n",
    "（结合课本图7.5）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848bc5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 非线性支持向量机与核函数\n",
    "\n",
    "* 主要特点是利用核技巧（kernel trick）\n",
    "* 核技巧不仅应用于支持向量机，而且应用于其他统计学习问题\n",
    "* 如果能用$\\mathbf{R}^n$中的一个超曲面将正负例正确分开，则称这个问题为非线性可分问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86feb3c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "* 非线性问题往往不好求解，所以希望能用解线性分类问题的方法解决这个问题\n",
    "* 所釆取的方法是进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250c24f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "\n",
    "* 定义从原空间到新空间的变换\n",
    "\n",
    "\\begin{equation*}\n",
    "z=\\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6dfa2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "* 原空间中的椭圆\n",
    "\n",
    "  \\begin{equation*}\n",
    "  w_1(x^{(1)})^2+w_2(x^{(2)})^2+b=0\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 变换成为\n",
    "* 新空间中的直线\n",
    "\n",
    "  \\begin{equation*}\n",
    "  w_1z^{(1)}+w_2z^{(2)}+b=0\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbc6ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "用线性分类方法求解非线性分类问题分为两步：\n",
    "\n",
    "* 首先使用一个变换将原空间的数据映射到新空间\n",
    "* 然后在新空间里用线性分类学习方法从训练数据中学习分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe16a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "在输入空间$\\mathbf{R}^N$中的超曲面模型对应于特征空间$\\mathcal{H}$中的超平面模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46001ad0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "* 映射函数\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\phi(x):\\mathcal{X}\\to \\mathcal{H}\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 核函数\n",
    "\n",
    "  \\begin{equation*}\n",
    "  K(x,z)=\\phi(x)\\cdot \\phi(z)\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64781d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "* 核技巧的想法是，在学习与预测中只定义核函数$K(x,z)$，而不显式地定义映射函数$\\phi$\n",
    "* 通常，直接计算$K(x,z)$比较容易，而通过$\\phi(x)$和$\\phi(z)$计算$K(x,z)$并不容易\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db26f901",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "* 课本例7.3\n",
    "\n",
    "  \\begin{equation*}\n",
    "  K(x,z)=(x\\cdot z)^2\n",
    "  \\end{equation*}\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  (x\\cdot z)^2=(x^{(1)}z^{(1)})^2+2x^{(1)}z^{(1)}x^{(2)}z^{(2)}+(x^{(2)}z^{(2)})^2\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 映射：\n",
    "\n",
    "  \\begin{align*}\n",
    "  \\phi(x)&=((x^{(1)})^2,\\sqrt{2}x^{(1)}x^{(2)},(x^{(2)})^2)^T\\\\\n",
    "  \\phi(x)&=\\frac{1}{\\sqrt{2}}((x^{(1)})^2-(x^{(2)})^2,2x^{(1)}x^{(2)},(x^{(1)})^2+(x^{(2)})^2)^T\\\\\n",
    "  \\phi(x)&=((x^{(1)})^2,x^{(1)}x^{(2)},x^{(1)}x^{(2)},(x^{(2)})^2)^T\n",
    "  \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef5bab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "#### 核技巧在支持向量机中的应用\n",
    "\n",
    "* 在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积\n",
    "* 在对偶问题的目标函数中的内积$x_i\\cdot x_j$可以用核函数$K(x_i,x_j)=\\phi(x_i)\\cdot \\phi(x_j)$来代替"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff10fb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "#### 核技巧在支持向量机中的应用\n",
    "\n",
    "* 对偶问题的目标函数成为\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  W(\\alpha)=\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum_{i=1}^N\\alpha_i\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955230fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "#### 核技巧在支持向量机中的应用\n",
    "\n",
    "* 分类决策函数式成为\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\begin{split}\n",
    "  f(x)&=\\mathrm{sign}\\biggl(\\sum_{i=1}^N \\alpha^*_iy_i\\phi(x_i)\\cdot\\phi(x)+b^*\\biggr)\\\\\n",
    "  &=\\mathrm{sign}\\biggl(\\sum_{i=1}^N \\alpha^*_iy_iK(x_i,x)+b^*\\biggr)\n",
    "  \\end{split}\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f2137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 核技巧\n",
    "\n",
    "#### 核技巧在支持向量机中的应用\n",
    "\n",
    "* 当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型\n",
    "* 学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数\n",
    "* 在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd746f9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 常用核函数\n",
    "\n",
    "* 多项式核函数（polynomial kernel function）\n",
    "\n",
    "  \\begin{equation*}\n",
    "  K(x,z)=(x\\cdot z+1)^2\n",
    "  \\end{equation*}\n",
    "  \n",
    "  对应的支持向量机是一个$p$次多项式分类器\n",
    "  \n",
    "* 高斯核函数（Gaussian kernel function）\n",
    "\n",
    "  \\begin{equation*}\n",
    "  K(x,z)=\\exp\\biggl(-\\frac{\\|x-z\\|^2}{2\\sigma^2}\\biggr)\n",
    "  \\end{equation*}\n",
    "  \n",
    "  对应的支持向量机是高斯径向基函数（radial basis function）分类器\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55cb635",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 常用核函数\n",
    "\n",
    "* 字符串核函数（string kernel function）\n",
    "* 有限字符表$\\Sigma$\n",
    "* 字符串$s$：$s(1)s(2)\\dotsc s(|s|)$\n",
    "* 所有长度为$n$的字符串的集合记作$\\Sigma^n$\n",
    "* subsequence的概念\n",
    "  \n",
    "  straddle - 横跨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e0468d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 常用核函数\n",
    "\n",
    "* 字符串核函数（string kernel function）\n",
    "* 例：\n",
    "\n",
    "  * 字符串`Nasdaq`及`lass das`\n",
    "  * 特征空间$\\mathcal{H}_3$的一维对应于字符串`asd`\n",
    "  * 在这一维上的值分别是\n",
    "  \n",
    "    \\begin{align*}\n",
    "    [\\phi_3(\\text{Nasdaq})]_\\text{asd}&=\\lambda^3\\\\\n",
    "    [\\phi_3(\\text{lass das})]_\\text{asd}&=2\\lambda^5\n",
    "    \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b0a5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 常用核函数\n",
    "\n",
    "* 字符串核函数（string kernel function）\n",
    "* 字符串核函数$k_n(s,t)$给出了字符串$s$和$t$中长度等于$n$的所有subsequence组成的特征向量的余弦相似度（cosine similarity）"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
