{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d42390",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# logistic回归与最大熵模型\n",
    "\n",
    "* logistic回归是统计学习中的经典分类方法\n",
    "* 最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型（maximum entropy model）\n",
    "* logistic回归模型与最大熵模型都属于对数线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3becc28f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## logistic回归模型\n",
    "\n",
    "### logistic分布\n",
    "\n",
    "* $X$是连续随机变量\n",
    "* 分布函数：\n",
    "\n",
    "  \\begin{equation*}\n",
    "  F(x)=P(X\\le x)=\\frac{1}{1+e^{-(x-\\mu)/s}}\n",
    "  \\end{equation*}\n",
    "\n",
    "* 密度函数：\n",
    "\n",
    "  \\begin{equation*}\n",
    "  f(x)=F'(x)=\\frac{e^{-(x-\\mu)/s}}{s\\bigl(1+e^{-(x-\\mu)/s}\\bigr)^2}\n",
    "  \\end{equation*}\n",
    "  \n",
    "* $\\mu$为位置参数，$s>0$为形状参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c79c47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## logistic回归模型\n",
    "\n",
    "* location parameter $\\mu$; scale parameter $s$\n",
    "\n",
    "pdf             |  cdf\n",
    ":-------------------------:|:-------------------------:\n",
    "![alt-text-1](img/Logisticpdfunction.svg \"title-1\")  |  ![alt-text-2](img/Logistic_cdf.svg \"title-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72061a40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 二项logistic regression模型\n",
    "\n",
    "* 参数化的logistic distribution\n",
    "  \\begin{align*}\n",
    "  P(Y=1|x)&=\\frac{\\exp(w\\cdot x +b)}{1+\\exp(w\\cdot x+b)}\\\\\n",
    "  P(Y=0|x)&=\\frac{1}{1+\\exp(w\\cdot x+b)}\n",
    "  \\end{align*}\n",
    "  \n",
    "* 将权值向量和输入向量加以扩充\n",
    "\n",
    "  \\begin{align*}\n",
    "  w&=(w^{(1)},w^{(2)},\\dotsc,w^{(n)},b)^T\\\\\n",
    "  x&=(x^{(1)},x^{(2)},\\dotsc,x^{(n)},1)^T\n",
    "  \\end{align*}\n",
    "  \n",
    "课堂提问：扩充带来的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18791c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 二项logistic regression模型\n",
    "\n",
    "* 一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值\n",
    "* 对数几率（log odds）或logit函数\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\mathrm{logit}(p)=\\log\\frac{p}{1-p}\n",
    "  \\end{equation*}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761c319",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 二项logistic regression模型\n",
    "\n",
    "  \n",
    "* 对logistic regression，有\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型\n",
    "\n",
    "课堂提问：$w\\cdot x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948b5d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 模型参数估计\n",
    "\n",
    "* 应用极大似然估计法估计模型参数\n",
    "* 似然函数为\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\prod_{i=1}^N[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}\n",
    "  \\end{equation*}\n",
    "\n",
    "课堂提问：极大似然估计法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aac405",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 模型参数估计\n",
    "\n",
    "* 对数似然函数为\n",
    "\n",
    "  \\begin{align*}\n",
    "  L(w)&=\\sum_{i=1}^N[y_i\\log\\pi(x_i)+(1-y_i)\\log(1-\\pi(x_i))]\\\\\n",
    "  &=\\sum_{i=1}^N\\biggl[y_i\\log\\frac{\\pi(x_i)}{1-\\pi(x_i)}+\\log(1-\\pi(x_i))\\biggr]\\\\\n",
    "  &=\\sum_{i=1}^N[y_i(w\\cdot x_i)-\\log(1+\\exp(w\\cdot x))]\n",
    "  \\end{align*}\n",
    "  \n",
    "* 对$L(w)$求极大值，得到$w$的估计值\n",
    "\n",
    "课堂提问：为什么考虑对数似然函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ace9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 多项logistic regression\n",
    "\n",
    "* $k=1,2,\\dotsc,K-1$\n",
    "\n",
    "  \\begin{equation*}\n",
    "  P(Y=k|x)=\\frac{\\exp(w_k\\cdot x)}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\cdot x)}\n",
    "  \\end{equation*}\n",
    "\n",
    "* $k=K$\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  P(Y=K|x)=\\frac{1}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\cdot x)}\n",
    "  \\end{equation*}\n",
    "  \n",
    "课堂提问：为什么只需要$K-1$个$w_k$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d5ec0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 最大熵模型（maximum entropy model）\n",
    "\n",
    "### 最大熵原理\n",
    "\n",
    "* 最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型\n",
    "* 通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ebbf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵原理\n",
    "\n",
    "* 直观地，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件\n",
    "* 在没有更多信息的情况下，那些不确定的部分都是“等可能的”\n",
    "* 最大熵原理通过熵的最大化来表示等可能性\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfbee10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 拉格朗日对偶性\n",
    "\n",
    "* 在约束最优化问题中，常常利用拉格朗日对偶性（Lagrange duality）将原始问题转换为对偶问题\n",
    "* 通过解对偶问题而得到原始问题的解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfa320",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 原始问题\n",
    "\n",
    "* 假设$f(x)$，$c_i(x)$，$h_j(x)$是定义在$\\mathbf{R}^n$上的连续可微函数\n",
    "\n",
    "* 考虑约束最优化问题\n",
    "\n",
    "  \\begin{align*}\n",
    "  \\min_{x\\in \\mathbf{R}^n}\\quad &f(x)\\\\\n",
    "  \\mathrm{s.t.}\\quad &c_i(x)\\le 0,\\quad i=1,2,\\dotsc,k\\\\\n",
    "  &h_j(x)=0,\\quad j=1,2,\\dotsc,l\n",
    "  \\end{align*}\n",
    "\n",
    "* 原始问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eff938",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 原始问题\n",
    "\n",
    "* 广义拉格朗日函数（generalized Lagrange function）\n",
    "\n",
    "  \\begin{equation*}\n",
    "  L(x,\\alpha,\\beta)=f(x)+\\sum_{i=1}^k\\alpha_i c_i(x)+\\sum_{j=1}^l\\beta_jh_j(x)\n",
    "  \\end{equation*}\n",
    " \n",
    "* $\\alpha_i$，$\\beta_j$是拉格朗日乘子，$\\alpha_i\\ge 0$（**注意$\\ge 0$**）\n",
    "\n",
    "* 考虑$x$的函数：\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\theta_P(x)=\\max_{\\alpha_i,\\beta:\\alpha_i\\ge 0}L(x,\\alpha,\\beta)\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8ac68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 原始问题\n",
    "\n",
    "* 假设给定某个$x$\n",
    "* 如果$x$违反原始问题的约束条件，那么就有\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\theta_P(x)=\\infty\n",
    "  \\end{equation*}\n",
    "  \n",
    "课堂提问：为什么$\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f6157",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 原始问题\n",
    "  \n",
    "* 约束$c_i(x)>0$，则可令$\\alpha_i\\to\\infty$\n",
    "\n",
    "  $h_j(x)\\neq 0$，则可令$\\beta_j$使$\\beta_j h_j(x)\\to\\infty$\n",
    "\n",
    "  将其余各$\\alpha_i$，$\\beta_j$均取为0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fb90c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 原始问题\n",
    "\n",
    "* 相反地，如果$x$满足约束条件，可知\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\theta_P(x)=f(x)\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 因此，\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\theta_P(x)=\n",
    "  \\begin{cases}\n",
    "  f(x),\\quad&\\text{$x$满足原始问题约束}\\\\\n",
    "  \\infty,\\quad&\\text{其他}\n",
    "  \\end{cases}\n",
    "  \\end{equation*}\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda3235",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 原始问题\n",
    "\n",
    "* 考虑极小化问题\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\min_x \\theta_P(x)=\\min_x \\max_{\\alpha_i,\\beta:\\alpha_i\\ge 0}L(x,\\alpha,\\beta)\n",
    "  \\end{equation*}\n",
    "  \n",
    "  * 是与原始最优化问题等价的，即它们有相同的解\n",
    "  * 课堂提问：有相同的解（为什么）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8647dc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 原始问题\n",
    "\n",
    "* 原始最优化问题表示为广义拉格朗日函数的极小极大问题\n",
    "* 定义原始问题的最优值\n",
    "\n",
    "  \\begin{equation*}\n",
    "  p^*=\\min_x\\theta_P(x)\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc490f58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 对偶问题\n",
    "\n",
    "* 定义\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\theta_D(\\alpha,\\beta)=\\min_x L(x,\\alpha,\\beta)\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 考虑极大化\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\max_{\\alpha_i,\\beta:\\alpha_i\\ge 0}\\theta_D(\\alpha,\\beta)=\\max_{\\alpha_i,\\beta:\\alpha_i\\ge 0}\\min_x L(x,\\alpha,\\beta)\n",
    "  \\end{equation*}\n",
    "  \n",
    "  * 称为广义拉格朗日函数的极大极小问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bfd0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 对偶问题\n",
    "\n",
    "* 可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题\n",
    "\n",
    "  \\begin{align*}\n",
    "  \\max_{\\alpha,\\beta}\\quad &\\theta_D(\\alpha,\\beta)\\\\\n",
    "  \\mathrm{s.t.}\\quad &\\alpha_i\\ge 0,\\quad i=1,2,\\dotsc,k\n",
    "  \\end{align*}\n",
    "  \n",
    "* 称为原始问题的对偶问题\n",
    "\n",
    "* 定义对偶问题的最优值\n",
    "\n",
    "  \\begin{equation*}\n",
    "  d^*=\\max_{\\alpha,\\beta:\\alpha_i\\ge 0}\\theta_D(\\alpha,\\beta)\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff73e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 原始问题和对偶问题的关系\n",
    "\n",
    "* 若原始问题和对偶问题都有最优值，则\n",
    "\n",
    "  \\begin{equation*}\n",
    "  d^*=\\max_{\\alpha,\\beta:\\alpha_i\\ge 0}\\min_x L(x,\\alpha,\\beta)\\le \\min_x\\max_{\\alpha_i,\\beta:\\alpha_i\\ge 0}L(x,\\alpha,\\beta)=p^* \n",
    "  \\end{equation*}\n",
    "\n",
    "* 证明\n",
    "    * 对任意的$\\alpha$（$\\alpha_i\\ge 0$，见广义拉格朗日函数的定义），$\\beta$和$x$，有\n",
    "\n",
    "      \\begin{equation*}\n",
    "      \\begin{split}\n",
    "      \\theta_D(\\alpha,\\beta)=\\min_x L(x,\\alpha,\\beta)&\\le L(x,\\alpha,\\beta)\\\\\n",
    "      &\\le \\max_{\\alpha_i,\\beta:\\alpha_i\\ge 0}L(x,\\alpha,\\beta)=\\theta_P(x)\n",
    "      \\end{split}\n",
    "      \\end{equation*}\n",
    "      \n",
    "    * 由于原始问题和对偶问题均有最优值，所以，\n",
    "    \n",
    "      \\begin{equation*}\n",
    "      \\max_{\\alpha,\\beta:\\alpha_i\\ge 0}\\theta_D(\\alpha,\\beta)\\le \\min_x\\theta_P(x)\n",
    "      \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0cf3ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 例6.1\n",
    "\n",
    "  * 假设随机变量$X$有5个取值$\\{A,B,C,D,E\\}$\n",
    "  * 要估计取各个值的概率$P(A),P(B),P(C),P(D),P(E)$\n",
    "  * $P(A)+P(B)=\\tfrac{3}{10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256b01e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 最大熵模型学习的最优化问题是\n",
    "\n",
    "  \\begin{align*}\n",
    "  \\min\\quad &-H(P)=\\sum_{i=1}^5 P(y_i)\\log P(y_i)\\\\\n",
    "  \\mathrm{s.t.}\\quad &P(y_1)+P(y_2)=\\frac{3}{10}\\\\\n",
    "  &\\sum_{i=1}^5 P(y_i)=1\n",
    "  \\end{align*}\n",
    "  \n",
    "  分别以$y_1,y_2,y_3,y_4,y_5$表示$A$，$B$，$C$，$D$和$E$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5260420",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 引进拉格朗日乘子$w_0,w_1$，定义拉格朗日函数\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\begin{split}\n",
    "  L(P,w)=\\sum_{i=1}^5 P(y_i)\\log P(y_i)&+w_1\\biggl(P(y_1)+P(y_2)-\\frac{3}{10}\\biggr)\\\\\n",
    "  &+w_0\\biggl(\\sum_{i=1}^5 P(y_i)-1\\biggr)\n",
    "  \\end{split}\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 根据拉格朗日对偶性，可以通过求解对偶最优化问题得到原始最优化问题的解，所以求解\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\max_w\\min_P L(P,w)\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1eed5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 先求解$L(P,w)$关于$P$的极小化问题\n",
    "\n",
    "  * 为此，固定$w_0,w_1$，求偏导数\n",
    "  \n",
    "    \\begin{align*}\n",
    "    \\frac{\\partial L(P,w)}{\\partial P(y_1)}&=1+\\log P(y_1)+w_1+w_0\\\\\n",
    "    \\frac{\\partial L(P,w)}{\\partial P(y_2)}&=1+\\log P(y_2)+w_1+w_0\\\\\n",
    "    \\frac{\\partial L(P,w)}{\\partial P(y_3)}&=1+\\log P(y_3)+w_0\\\\\n",
    "    \\frac{\\partial L(P,w)}{\\partial P(y_4)}&=1+\\log P(y_4)+w_0\\\\\n",
    "    \\frac{\\partial L(P,w)}{\\partial P(y_5)}&=1+\\log P(y_5)+w_0\n",
    "    \\end{align*}\n",
    "    \n",
    "  * 令各偏导数等于0，解得\n",
    "  \n",
    "    \\begin{align*}\n",
    "    &P(y_1)=P(y_2)=e^{-w_1-w_0-1}\\\\\n",
    "    &P(y_3)=P(y_4)=P(y_5)=e^{-w_0-1}\n",
    "    \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa29e3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* $L(P,w)$关于$P$的极小化问题\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\begin{split}\n",
    "  \\min_P L(P,w)&=L(P_w,w)\\\\&\n",
    "  =-2e^{-w_1-w_0-1}-3e^{-w_0-1}-\\frac{3}{10}w_1-w_0\n",
    "  \\end{split}\n",
    "  \\end{equation*}\n",
    "  \n",
    "* 再求解$L(P_w,w)$关于$w$的极大化问题$\\max_w L(P_w,w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7242e77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 分别求$L(P_w,w)$对$w_0,w_1$的偏导数并令其为0，得到\n",
    "\n",
    "  \\begin{align*}\n",
    "  e^{-w_1-w_0-1}&=\\frac{3}{20}\\\\\n",
    "  e^{-w_0-1}&=\\frac{7}{30}\n",
    "  \\end{align*}\n",
    "  \n",
    "* 于是得到所要求的概率分布为\n",
    "\n",
    "  \\begin{align*}\n",
    "  &P(y_1)=P(y_2)=\\frac{3}{20}\\\\\n",
    "  &P(y_3)=P(y_4)=P(y_5)=\\frac{7}{20}\n",
    "  \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8510ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 例6.1扩展：$P(A)+P(C)=\\tfrac{1}{2}$\n",
    "    \n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "L(P,w)=\\sum_{i=1}^5 P(y_i)\\log P(y_i)&+w_2\\biggl(P(y_1)+P(y_3)-\\frac{1}{2}\\biggr)\\\\\n",
    "&+w_1\\biggl(P(y_1)+P(y_2)-\\frac{3}{10}\\biggr)\\\\\n",
    "&+w_0\\biggl(\\sum_{i=1}^5 P(y_i)-1\\biggr)\n",
    "\\end{split}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f4ade",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 例6.1扩展\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(P,w)}{\\partial P(y_1)}&=1+\\log P(y_1)+w_2+w_1+w_0\\\\\n",
    "\\frac{\\partial L(P,w)}{\\partial P(y_2)}&=1+\\log P(y_2)+w_1+w_0\\\\\n",
    "\\frac{\\partial L(P,w)}{\\partial P(y_3)}&=1+\\log P(y_3)+w_2+w_0\\\\\n",
    "\\frac{\\partial L(P,w)}{\\partial P(y_4)}&=1+\\log P(y_4)+w_0\\\\\n",
    "\\frac{\\partial L(P,w)}{\\partial P(y_5)}&=1+\\log P(y_5)+w_0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1b26c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 例6.1扩展\n",
    "\n",
    "\\begin{align*}\n",
    "&P(y_1)=e^{-w_2-w_1-w_0-1}\\\\\n",
    "&P(y_2)=e^{-w_1-w_0-1}\\\\\n",
    "&P(y_3)=e^{-w_2-w_0-1}\\\\\n",
    "&P(y_4)=P(y_5)=e^{-w_0-1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ced0e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 例6.1扩展\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\min_P L(P,w)=&L(P_w,w)\\\\\n",
    "=&-e^{-w_2-w_1-w_0-1}-e^{-w_1-w_0-1}\\\\\n",
    "&-e^{-w_2-w_0-1}-3e^{-w_0-1}-\\frac{1}{2}w_2-\\frac{3}{10}w_1-w_0\n",
    "\\end{split}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a98829",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 例6.1扩展\n",
    "\n",
    "\\begin{align*}\n",
    "e^{-w_2-w_1-w_0-1}+e^{-w_2-w_0-1}&=\\frac{1}{2}\\\\\n",
    "e^{-w_2-w_1-w_0-1}+e^{-w_1-w_0-1}&=\\frac{3}{10}\\\\\n",
    "e^{-w_2-w_1-w_0-1}+e^{-w_1-w_0-1}+e^{-w_2-w_0-1}+3e^{-w_0-1}&=1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992befb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 最大熵模型的学习\n",
    "\n",
    "* 例6.1扩展\n",
    "\n",
    "\\begin{align*}\n",
    "P(A)+P(C)&=\\frac{1}{2}\\\\\n",
    "P(A)+P(B)&=\\frac{3}{10}\\\\\n",
    "P(A)+P(B)+3P(C)&=1\n",
    "\\end{align*}\n",
    "\n",
    "因此有：\n",
    "\n",
    "\\begin{align*}\n",
    "P(C)&=\\frac{7}{20}\\\\\n",
    "P(A)&=\\frac{3}{20}\\\\\n",
    "P(B)&=\\frac{3}{20}\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
