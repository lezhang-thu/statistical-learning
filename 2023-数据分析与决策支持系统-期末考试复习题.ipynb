{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09df61d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. 决策树\n",
    "\n",
    "考虑如下的训练数据集，请回答：\n",
    "\n",
    "* 决策树分类在特征选择时，选择特征的准则中常用的信息增益及信息增益比分别是什么；\n",
    "* 若对如下的数据集构造决策树时，根结点将会根据信息增益选取特征。请你计算各个特征的信息增益。注意，仅需要写出数学表达式，不需要进行化简，但需写明计算过程。\n",
    "    \n",
    "表1 应聘人员情况数据表\n",
    "    \n",
    "|       | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |\n",
    "| :----:| :-----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n",
    "| 身体  | 0|\t0|\t1|\t1|\t1|\t0|\t1|\t1|\t1|\t0|\n",
    "| 业务  | 1|\t3|\t2|\t1|\t2|\t1|\t1|\t1|\t3|\t2|\n",
    "| 潜力  | 3|\t1|\t2|\t3|\t3|\t2|\t2|\t1|\t1|\t1|\n",
    "| 分类  | -1|\t-1|\t-1|\t-1|\t-1|\t-1|\t1|\t1|\t-1|\t-1|\n",
    "\n",
    "**复习补充习题：**\n",
    "\n",
    "* 统计学习方法（第2版）：例5.2、例5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f0506",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. 朴素贝叶斯分类算法\n",
    "\n",
    "一个朴素贝叶斯分类器被构造在具有10个属性（仅考虑属性取值为离散型的情况）的数据集上。\n",
    "\n",
    "* 请简要阐述朴素贝叶斯分类器的思路；\n",
    "* 现在有一条数据元组需要被分类，但是其上只有8个属性的信息。在这种情况下，如何使用该分类器对其进行分类。给出解决方法并简要阐述原因。\n",
    "\n",
    "**解答：**\n",
    "\n",
    "在10个属性的信息都知道的情况下，考虑的为$P(C_i|X=x_1,\\dotsc,x_{10})$。对于只有8个属性信息的数据元组，设这8个属性为$X'$，则相应考虑的为$P(C_i|X')$。\n",
    "\n",
    "对$P(C_i|X')$的计算仍然可使用贝叶斯定理及条件独立性假设。\n",
    "\n",
    "3. 考虑如下的数据集，其有三个布尔属性$a$、$b$、$c$，标签为$K$。\n",
    "\n",
    "| a        | b      | c     | K     |\n",
    "| :---:    | :----: | :---: | :---: | \n",
    "| 1        | 0      | 1     | 1     |\n",
    "| 1        | 1      | 1     | 1     |\n",
    "| 0        | 1      | 1     | 0     |\n",
    "| 1        | 1      | 0     | 0     |\n",
    "| 1        | 0      | 1     | 0     |\n",
    "| 0        | 0      | 0     | 1     |\n",
    "| 0        | 0      | 0     | 1     |\n",
    "| 0        | 0      | 1     | 0     |\n",
    "\n",
    "若这里将使用朴素贝叶斯分类器来对$K$的值进行预测，基于$a$、$b$、$c$这3个属性上的取值。请回答如下的问题：\n",
    "\n",
    "* 朴素贝叶斯分类器中做出了什么样的假设（即何谓朴素）；这样的假设带来了什么样的简化；\n",
    "* 根据朴素贝叶斯分类器，请问$P(K=1|a=1,b=1,c=0)$为多少。请给出结果及相应的计算过程；\n",
    "* 根据朴素贝叶斯分类器，请问$P(K=1|a=1,b=1)$为多少。请给出结果及相应的计算过程。\n",
    "\n",
    "**解答：**\n",
    "\n",
    "a. \n",
    "\n",
    "设数据点的形式为$X=(x_1,x_2,\\dotsc,x_n)$，类别标签有$m$种，即$C_1,C_2,\\dotsc,C_m$。贝叶斯算法中需要求解：$P(X|C_i)$。但因为$X=(x_1,x_2,\\dotsc,x_n)$，所以，计算$P(X|C_i)$将极为复杂，因此朴素贝叶斯算法中做出了所谓的条件独立性假设，也就是说：\n",
    "\n",
    "$$\n",
    "P(X|C_i)=P(x_1|C_i)P(x_2|C_i)\\dotsb P(x_n|C_i)\n",
    "$$\n",
    "\n",
    "这意味着，在给定一个类别的情况下，一个属性上的取值，和别的属性上的取值是独立的。这也是为什么朴素贝叶斯算法被称为是朴素的，因为做出如上的、简化计算的假设。\n",
    "\n",
    "b.\n",
    "\n",
    "根据贝叶斯定理，有：\n",
    "\n",
    "\\begin{equation*}\n",
    "P(K=1|a=1,b=1,c=0)=\\frac{P(a=1,b=1,c=0|K=1)P(K=1)}{P(a=1,b=1,c=0)}\n",
    "\\end{equation*}\n",
    "\n",
    "首先计算$P(a=1,b=1,c=0)=\\tfrac{1}{8}$；其次$P(K=1)=\\tfrac{4}{8}=\\tfrac{1}{2}$。\n",
    "\n",
    "根据条件独立性假设，有：\n",
    "\n",
    "$$\n",
    "P(a=1,b=1,c=0|K=1)=P(a=1|K=1)P(b=1|K=1)P(c=0|K=1)\n",
    "$$\n",
    "\n",
    "其中，$P(a=1|K=1)=\\tfrac{2}{4}=\\tfrac{1}{2}$，$P(b=1|K=1)=\\tfrac{1}{4}$，$P(c=0|K=1)=\\tfrac{2}{4}=\\tfrac{1}{2}$。综上，可以得到：\n",
    "\n",
    "$$\n",
    "P(K=1|a=1,b=1,c=0)=\\frac{\\frac{1}{2}\\frac{1}{4}\\frac{1}{2}\\frac{1}{2}}{\\frac{1}{8}}=\\frac{1}{4}\n",
    "$$\n",
    "\n",
    "c.\n",
    "\n",
    "根据贝叶斯定理，有：\n",
    "\n",
    "\\begin{equation*}\n",
    "P(K=1|a=1,b=1)=\\frac{P(a=1,b=1|K=1)P(K=1)}{P(a=1,b=1)}\n",
    "\\end{equation*}\n",
    "\n",
    "首先计算$P(a=1,b=1)=\\tfrac{2}{8}=\\tfrac{1}{4}$；其次$P(K=1)=\\tfrac{1}{2}$。\n",
    "\n",
    "根据条件独立性假设，有：\n",
    "\n",
    "$$\n",
    "P(a=1,b=1|K=1)=P(a=1|K=1)P(b=1|K=1)\n",
    "$$\n",
    "\n",
    "其中，$P(a=1|K=1)=\\tfrac{1}{2}$，$P(b=1|K=1)=\\tfrac{1}{4}$。综上，可以得到：\n",
    "\n",
    "$$\n",
    "P(K=1|a=1,b=1,c=0)=\\frac{\\frac{1}{2}\\frac{1}{4}\\frac{1}{2}}{\\frac{1}{4}}=\\frac{1}{4}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**复习补充习题：**\n",
    "\n",
    "* 统计学习方法（第2版）：例4.1\n",
    "* 数据挖掘：概念与技术（第3版）：Example 8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea14864",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4. 高斯混合模型及EM算法\n",
    "\n",
    "4.1 结合\"数据挖掘：概念与技术（第3版）：Example 12.8\"，试证明，对于高斯分布$\\mathcal{N}(x|\\mu,\\sigma)$，有：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial }{\\partial\\mu}\\log \\mathcal{N}(x|\\mu,\\sigma)=\\frac{x-\\mu}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "**解答：**\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial }{\\partial\\mu}\\log \\mathcal{N}(x|\\mu,\\sigma)&=\\frac{\\partial }{\\partial\\mu}\\log \\biggl(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\biggr)\\\\\n",
    "&=\\frac{\\partial }{\\partial\\mu}\\biggl(\\log \\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{(x-\\mu)^2}{2\\sigma^2}\\biggr)\\\\\n",
    "&=\\frac{x-\\mu}{\\sigma^2}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "4.2 阅读英文版\"数据挖掘：概念与技术（第3版）：Using a Mixture of Parametric Distributions\"及其中的\"Example 12.12\"，请撰写一小段文字（不超过300字，不含标点符号）表达你对该小节内容的理解。\n",
    "\n",
    "4.3 考虑这样的具有两个混合分量的单变量高斯混合模型：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "z&\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n",
    "x\\mid z=k&\\sim \\mathcal{N}(\\mu_k,\\sigma_k)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "现在定义$\\phi =\\{\\theta,\\mu_0,\\sigma_0,\\mu_1,\\sigma_1\\}$，请回答如下的问题：\n",
    "\n",
    "1. 若当前的参数估计为$\\phi^{(t)}$，请给出观测变量$x$的某一个观察值$x^{(i)}$的概率密度：\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x^{(i)};\\phi^{(t)})\n",
    "\\end{equation*}\n",
    "\n",
    "提示：请考虑$p(x;\\phi)=\\sum_z p(x,z;\\phi)$，以及$p(x,z;\\phi)=p(x|z;\\phi-\\{\\theta\\})p(z;\\theta)$\n",
    "\n",
    "**解答：**\n",
    "\n",
    "$$\n",
    "p(x;\\phi)=\\theta \\mathcal{N}(x;\\mu_1,\\sigma_1)+(1-\\theta)\\mathcal{N}(x;\\mu_0,\\sigma_0)\n",
    "$$\n",
    "\n",
    "2. 在EM算法的E-step，为了让如下的公式成立：\n",
    "\n",
    "\\begin{align*}\n",
    "\\log p(x^{(i)};\\phi^{(t)})&=\\sum_{z^{(i)}} Q(z^{(i)})\\log \\frac{p(x^{(i)},z^{(i)};\\phi^{(t)})}{Q(z^{(i)})}\n",
    "\\end{align*}\n",
    "\n",
    "其中，$Q(z^{(i)})$为$z^{(i)}$的某一个概率分布，那么，$Q(z^{(i)})$的形式应该是怎么样的，以及为什么$Q(z^{(i)})$的形式应该是如此。\n",
    "\n",
    "提示：请分别考虑$z^{(i)}=0$和$z^{(i)}=1$，然后，分别计算$Q(z^{(i)}=0)$和$Q(z^{(i)}=1)$。\n",
    "\n",
    "**解答：**\n",
    "\n",
    "$$\n",
    "Q(z^{(i)})=p(z^{(i)}|x^{(i)};\\phi^{(t)})\n",
    "$$\n",
    "\n",
    "考虑$z^{(i)}=0$，有：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "Q(z^{(i)}=0)&=p(z^{(i)}=0|x^{(i)};\\phi^{(t)})\\\\\n",
    "&=\\frac{p(z^{(i)}=0,x^{(i)};\\phi^{(t)})}{p(x^{(i)};\\phi^{(t)})}\\\\\n",
    "&=\\frac{(1-\\theta^{(t)})\\mathcal{N}(x^{(i)};\\mu^{(t)}_0,\\sigma^{(t)}_0)}{\\theta^{(t)} \\mathcal{N}(x^{(i)};\\mu^{(t)}_1,\\sigma^{(t)}_1)+(1-\\theta^{(t)})\\mathcal{N}(x^{(i)};\\mu^{(t)}_0,\\sigma^{(t)}_0)}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "因为$Q(z^{(i)})$为$z^{(i)}$的一个概率分布，所以，\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "Q(z^{(i)}=1)&=1-Q(z^{(i)}=0)\\\\\n",
    "&=\\frac{\\theta^{(t)} \\mathcal{N}(x^{(i)};\\mu^{(t)}_1,\\sigma^{(t)}_1)}{\\theta^{(t)} \\mathcal{N}(x^{(i)};\\mu^{(t)}_1,\\sigma^{(t)}_1)+(1-\\theta^{(t)})\\mathcal{N}(x^{(i)};\\mu^{(t)}_0,\\sigma^{(t)}_0)}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "3. 令$r^{(i)}=Q(z^{(i)}=1)$，那么，请问：在EM算法的M-step，$\\theta,\\mu_1,\\sigma_1$这三个参数的更新规则各自是怎样的。\n",
    "\n",
    "**解答：**\n",
    "\n",
    "令$r^{(i)}=Q(z^{(i)}=1)$，那么$Q(z^{(i)}=0)=1-r^{(i)}$。EM算法的M-step，需要最大化的目标$L(\\phi)$为：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\sum^N_{i=1}\\sum_{z^{(i)}} Q(z^{(i)})\\log p(x^{(i)},z^{(i)};\\phi)\\\\\n",
    "=&\\sum^N_{i=1}\\Bigl((1-r^{(i)})\\log\\bigl((1-\\theta) \\mathcal{N}(x^{(i)};\\mu_0,\\sigma_0)\\bigl)+r^{(i)}\\log \\bigl(\\theta\\mathcal{N}(x^{(i)};\\mu_1,\\sigma_1)\\bigr)\\Bigr)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "因为$\\tfrac{\\partial }{\\partial\\mu}\\log \\mathcal{N}(x|\\mu,\\sigma)=\\tfrac{x-\\mu}{\\sigma^2}$，所以，\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L(\\phi)}{\\partial \\mu_1}=\\sum^N_{i=1}r^{(i)}\\frac{x^{(i)}-\\mu_1}{\\sigma^2_1}\n",
    "\\end{equation*}\n",
    "\n",
    "令$\\tfrac{\\partial L(\\phi)}{\\partial \\mu_1}=0$，得到：\n",
    "\n",
    "$$\n",
    "\\mu_1=\\frac{\\sum^N_{i=1}r^{(i)}x^{(i)}}{\\sum^N_{i=1}r^{(i)}}\n",
    "$$\n",
    "\n",
    "同时，因为：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial }{\\partial\\sigma}\\log \\mathcal{N}(x|\\mu,\\sigma)&=\\frac{\\partial }{\\partial\\sigma}\\log \\biggl(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\biggr)\\\\\n",
    "&=\\frac{\\partial }{\\partial\\sigma}\\biggl(\\log \\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{(x-\\mu)^2}{2\\sigma^2}\\biggr)\\\\\n",
    "&=-\\frac{1}{\\sigma}-\\frac{(x-\\mu)^2}{2}(-2)\\sigma^{-3}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "所以，\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(\\phi)}{\\partial \\sigma_1}&=\\sum^N_{i=1}r^{(i)}\\biggl(-\\frac{1}{\\sigma_1}-\\frac{(x^{(i)}-\\mu_1)^2}{2}(-2)\\sigma^{-3}_1\\biggr)\\\\\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "令$\\tfrac{\\partial L(\\phi)}{\\partial \\sigma_1}=0$，得到：\n",
    "\n",
    "$$\n",
    "\\sum^N_{i=1}r^{(i)}\\biggl(-\\sigma^{2}_1+(x^{(i)}-\\mu_1)^2\\biggr)=0\n",
    "$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\n",
    "\\sigma^{2}_1=\\frac{\\sum^N_{i=1}r^{(i)}(x^{(i)}-\\mu_1)^2}{\\sum^N_{i=1}r^{(i)}}\n",
    "$$\n",
    "\n",
    "令$\\tfrac{\\partial L(\\phi)}{\\partial \\theta}=0$，得到：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{\\partial L(\\phi)}{\\partial \\theta}&=\\sum^N_{i=1}\\Bigl((1-r^{(i)})\\frac{1}{1-\\theta}(-1)+r^{(i)}\\frac{1}{\\theta}\\Bigr)\\\\\n",
    "&=0\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "也就是：\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\sum^N_{i=1}\\bigl((-1+r^{(i)})\\theta+r^{(i)}(1-\\theta)\\bigr)\\\\\n",
    "=&\\sum^N_{i=1}\\bigl(-\\theta+r^{(i)}\\bigr)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "所以，\n",
    "\n",
    "$$\n",
    "\\theta=\\frac{\\sum^N_{i=1}r^{(i)}}{N}\n",
    "$$\n",
    "\n",
    "**复习补充习题：**\n",
    "\n",
    "* 统计学习方法（第2版）：例9.1，其中的公式9.5至公式9.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6402bdc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5. 提升方法\n",
    "\n",
    "统计学习方法（第2版）：8.2 AdaBoost算法的训练误差分析\n",
    "\n",
    "5.1 请解释为什么如下的公式成立：\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_i e^{-\\sum_{m=1}^M\\alpha_my_iG_m(x_i)}=\\sum_i w_{1i}\\prod_{i=1}^Me^{-\\alpha_my_iG_m(x_i)}\n",
    "$$\n",
    "\n",
    "5.2 请解释为什么如下的公式成立：\n",
    "\n",
    "$$\n",
    "\\sum_i w_{1i}\\prod_{i=1}^Me^{-\\alpha_my_iG_m(x_i)}=Z_1\\sum_i w_{2i}\\prod_{i=2}^Me^{-\\alpha_my_iG_m(x_i)}\n",
    "$$\n",
    "\n",
    "5.3 定义$e_m=\\sum^N_{i=1} w_{mi}I(G_m(x_i)\\neq y_i)$，请用简要的文字解释（注意文字阐述，不要进行数学推导）阐述为什么如下的公式成立：\n",
    "\n",
    "$$\n",
    "Z_m=(1-e_m)e^{-\\alpha_m}+e_me^{\\alpha_m}\n",
    "$$\n",
    "\n",
    "**复习补充习题：**\n",
    "\n",
    "* 统计学习方法（第2版）：例8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3d76b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "6. 聚类方法\n",
    "\n",
    "给定数据集$\\{0,2,4,6,24,26\\}$，现在初始化$k$-means聚类算法的两个簇的中心$c_1=2,c_2=4$，注意$k$-means中$k$在这里值为2。请回答如下问题：\n",
    "\n",
    "a.\n",
    "\n",
    "1. 请简要阐述$k$-means的基本思路；\n",
    "2. 一次$k$-means的迭代之后，$c_1$、$c_2$分别为多少。请计算，并简要阐述计算过程；\n",
    "3. 第二次$k$-means的迭代之后，$c_1$、$c_2$分别为多少。请计算，并简要阐述计算过程；\n",
    "\n",
    "b.\n",
    "\n",
    "1. 请简要阐述$k$-medoids的基本思路；\n",
    "2. 若$k=2$，若初始的代表对象为$\\{0,6\\}$，那么初始的簇为什么；\n",
    "3. 考虑$k$-medoids的第1次迭代过程，请计算第1次迭代后所选择的代表对象以及相应的簇（注意，这里我们仅考虑让total cost $S$下降最多的、交换代表对象的选择）。\n",
    "\n",
    "**解答：**\n",
    "\n",
    "b.\n",
    "\n",
    "2.\n",
    "\n",
    "$\\{0,2,4,6,24,26\\}$距离$0$的距离分别为：$0,2,4,6,24,26$\n",
    "\n",
    "$\\{0,2,4,6,24,26\\}$距离$6$的距离分别为：$6,4,2,0,18,20$\n",
    "\n",
    "所以，初始簇为：$\\{0,2\\}$，$\\{4,6,24,26\\}$\n",
    "\n",
    "总的损失$S=0+2+2+0+18+20=42$。\n",
    "\n",
    "3.\n",
    "\n",
    "$k$-medoids的第1次迭代过程：\n",
    "\n",
    "代表对象为$\\{2,6\\}$时，$S=2+0+2+0+18+20=42$\n",
    "\n",
    "代表对象为$\\{4,6\\}$时，$S=4+2+0+0+18+20=44$\n",
    "\n",
    "代表对象为$\\{24,6\\}$时，$S=6+4+2+0+0+2=14$\n",
    "\n",
    "代表对象为$\\{26,6\\}$时，$S=6+4+2+0+2+0=14$\n",
    "\n",
    "代表对象为$\\{0,2\\}$时，$S=0+0+2+4+22+24=52$\n",
    "\n",
    "代表对象为$\\{0,4\\}$时，$S=0+2+0+2+20+22=46$\n",
    "\n",
    "代表对象为$\\{0,24\\}$时，$S=0+2+4+6+0+2=14$\n",
    "\n",
    "代表对象为$\\{0,26\\}$时，$S=0+2+4+6+2+0=14$\n",
    "\n",
    "所以，第1次迭代后所选择的代表对象为$\\{0,24\\}$，或者$\\{0,26\\}$，或者$\\{24,6\\}$，或者$\\{26,6\\}$，相应的簇为：$\\{0,2,4,6\\}$，$\\{24,26\\}$\n",
    "\n",
    "注：题目设置不严谨，导致出现多种替换方案均达到了$14$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af665dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "7. 感知机\n",
    "\n",
    "考虑正例点$x_1=(1,2)^T,x_2=(2,3)^T,x_3=(3,3)^T$，负例点$x_4=(2,1)^T,x_5=(3,2)^T$，请采用感知机学习算法的原始形式求解相应的感知机模型。\n",
    "\n",
    "（算法扫描数据的顺序为$x_1,x_2,\\dotsc,x_5$。）\n",
    "\n",
    "<center><img src=\"img/sample-final_exam.png\" width=\"30%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1db4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "8. 请回答如下问题：\n",
    "\n",
    "* 简述$k$-最近邻算法；\n",
    "* 对于下表中所示的样本集，分别根据1-最近邻、3-最近邻和5-最近邻算法，对数据点$X=5.0$分类。\n",
    "\n",
    "\n",
    "| X      | Y     |\n",
    "| :----: | :---: |\n",
    "| 0.5    | -     |\n",
    "| 3.0    | -     |\n",
    "| 4.5    | +     |\n",
    "| 4.6    | +     |\n",
    "| 4.9    | +     |\n",
    "| 5.2    | -     |\n",
    "| 5.3    | -     |\n",
    "| 5.7    | +     |\n",
    "| 7.0    | -     |\n",
    "| 9.5    | -     |"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
